\documentclass[11pt]{article}
\input{preamble.tex}

\title{Sampling thermodynamic ensembles of molecular systems with generative neural networks: Will integrating physics-based models close the generalization gap?}
\author{Grant M. Rotskoff\\
Department of Chemistry, Stanford, CA USA 94305\\
Institute for Computational and Mathematical Engineering, Stanford, CA USA 94305\\
\texttt{rotskoff@stanford.edu}}
\date{\today}

\usepackage{fancyhdr}

\fancypagestyle{fancy}{%
\fancyhf{}% clear all header and footer fields
\renewcommand{\headrulewidth}{0pt}%
\renewcommand{\footrulewidth}{0pt}%
}

\fancypagestyle{plain}{%
\fancyhf{}% clear all header and footer fields
\fancyfoot[L]{\textcolor{darkblue}{Last $\Delta$ = \today}} % except the center
\renewcommand{\headrulewidth}{0pt}%
\renewcommand{\footrulewidth}{0pt}%
}



\begin{document}

\maketitle

\begin{abstract}
If the promise of generative modeling techniques is realized, it may fundamentally change how we carry out molecular simulation.
The suite of techniques and models collectively termed ``generative AI'' includes many different classes of models built for varied types of data, from natural language to images.
Recent advances in the machine learning literature that construct ever better generative models, though, do not contend with the challenges unique to complex, molecular systems. 
To generate a statistically likely molecular configuration, many correlated degrees of freedom must be sampled together, while also satisfying the strong constraints of chemical physics. 
Recent efforts to develop generative models for biomolecular systems have shown spectacular results in some cases---nevertheless, some simple systems remain out of reach with our present methodology.
Arguably, the central concern is data efficiency: we should aim to train models that can meaningfully generalize beyond their training data and hence facilitate discovery.
In this review, we discuss methods and future directions for directly incorporating physics-based models into generative neural networks, which we believe is a crucial step for addressing the limitations of the current toolkit.
\end{abstract}


\section{Introduction and background}
\label{sec:intro}


Supervised machine learning has a rich history in the chemical sciences~\cite{lederberg_applications_1969}. 
For many decades, fitting linear and nonlinear regression models to experimental data has been a central modality of data-driven science.
In the last decade, both the complexity and expressiveness of the available models have grown rapidly: as the library of neural network architectures expanded, models that are well-calibrated for chemical and dynamical data have been widely and enthusiastically deployed in experimental and theoretical chemistry~\cite{mater_deep_2019}. 
While these algorithmic innovations have spurred substantive progress in cheminformatics~\cite{mater_deep_2019} and the physical sciences more broadly~\cite{carleo_machine_2019}, one cannot discount the formative role played by increasingly powerful computational resources~\cite{amodei_ai_2018} alongside sophisticated and reliable software for automatic differentiation and optimization~\cite{paszke_pytorch_2019, bradbury_jax_2018, broughton_tensorflow_2020}. 
Together, these developments have led to an active community of research designing ever-better networks, optimization strategies, and data curation to further improve the performance of data driven models.

A comparably new modeling paradigm is emerging alongside the developments in supervised learning due to the extraordinary developments in generative modeling.
Often called ``unsupervised learning'' as a contrast with regression or classification tasks on labelled data, generative models seek to directly emulate data.
For the purposes of this review, we will define a generative model to be any model that is parameterized to draw samples from a target probability distribution.
Information about a target probability distribution is either accessible through a data set of samples drawn from the distribution or, alternatively, through the unnormalized statistical likelihood function, which for a Gibbs-Boltzmann distribution is simply the energy.  
Generative models have attracted enormous attention for applications outside of science, such as text-to-image generation with denoising diffusion models~\cite{song_score-based_2022,ho_denoising_2020} and autoregressive text generation with large language models (LLMs)~\cite{vaswani_attention_2017}. 
The natural question, of course, is whether or not these approaches can amplify and accelerate scientific inquiry in the chemical context.

\begin{figure*}
\begin{center}
\includegraphics[width=0.9\linewidth]{schematic_v2.jpg}
\end{center}
\caption{Schematic overview of different types of models under consideration. (a) Denoising diffusion models. (b) Continuous normalizing flows and flow-matching models. (c) Normalizing flows based on coupling layers and compositional invertible maps.}
\label{fig:models}
\end{figure*}

In this review, we seek to articulate the potentialities and challenges of using generative models to sample finite temperature equilibrium probability distributions of chemical systems in the condensed phase.
We emphasize that this task is considerably different from, for example, protein structure prediction using AlphaFold~\cite{jumper_highly_2021, watson_novo_2023}, which predicts the crystal structure but does not capture molecular fluctuations.
Perhaps the defining challenge facing the generative modeling paradigm is \emph{generalization}: in many complex scientific tasks, large repositories of training data simply do not exist and generative models must instead be built from smaller data sets than those used for image generation applications or LLMs; in the latter case, empirical scaling laws show that increasing amounts of data improve the test loss as a power law~\cite{kaplan_scaling_2020, bahri_explaining_2021}.
If a generative model can only effectively sample molecular configurations that are very close to those provided in an initial data set, then little useful information can be gained from sampling the generative model. 
Nevertheless, researchers are actively building strategies to augment data, improve transferability, and enhance generalization by integrating generative models into enhanced sampling schemes already widely used in molecular simulation.

To provide a comprehensive overview of the challenges and opportunities afforded by generative models, we will first review current modeling paradigms, neural network architectures, and training approaches, without specializing to molecular systems. 
We will then discuss efforts to incorporate \emph{physical inductive bias}, that is, natural physical invariance and equivariance into generative neural networks, which promises to improve the quality of the generated samples and reduce training effort.
Our central focus will be on the question of generalization, and we will discuss several distinct strategies that seek to improve the capacity of these models for discovery, a question that remains formative for this new and developing field.
Finally, while there have been many efforts to incorporate generative models into the framework of existing enhanced sampling algorithms, for example, replica exchange~\cite{invernizzi_skipping_2022, wang_data_2022}, here we focus only on algorithms that aim to generate the entire molecular structure \emph{de novo}.
For an excellent review on enhanced sampling methods that incorporate machine learning, see Ref.~\cite{mehdi_enhanced_2023}.


\section{Generative modeling for molecular systems}

For a molecular system in thermal equilibrium at constant temperature, volume, and particle number, we know from elementary statistical mechanics~\cite{chandler_introduction_1987} that the probability density of a given configuration can be written as,
\begin{equation}
    \rhoeq(\xb) = Z^{-1} e^{-\beta U(\xb)},
    \label{eq:rhoeq}
\end{equation}
where $\beta=1/(k_{\rm B} T)$, $U$ is the potential energy function for the system, and $Z = \int_{\Omega} e^{-\beta U(\xb)} d\xb$ is the normalization constant for the distribution, also called the partition function. 
Of course, it is not possible to evaluate $\rhoeq$ directly because the partition function is ``intractable'', requiring that we compute a high-dimensional integral which is typically impossible.

Because we have accurate models of the energy $U$ from either electronic structure methods or classical approximations collectively referred to as molecular force fields~\cite{case_amber_2005, vanommeslaeghe_charmm_2010}, we usually draw samples from~\eqref{eq:rhoeq} with Markov chain Monte Carlo (MCMC) sampling algorithms.
Access to a collection of configurations allows us to estimate observable properties of the system by evaluating an ``empirical expectation'' or sample average,
\begin{equation}
\begin{aligned}
    \avg{\mathcal{A}} &= Z^{-1} \int_{\Omega} \mathcal{A}(\xb) e^{-\beta U(\xb)} d\xb, \\
    &\approx \frac1n \sum_{i=1}^n \mathcal{A}(\xb_i), \quad \xb_i \sim \rhoeq,
    \label{eq:avg}
\end{aligned}
\end{equation}
for any observable function $\mathcal{A}.$
In principle, estimating \eqref{eq:avg} is straightforward, requiring only a direct MCMC simulation or a molecular dynamics (MD) simulation in which the samples have been appropriately ``metropolized'' to ensure Boltzmann statistics~\cite{frenkel_understanding_2002}. 
In practice, such estimates rarely converge quickly because molecular systems often have metastable probability distributions. 
The amount of simulation time and hence computational effort required for MD or MCMC to ``mix'' between the two metastable configurations is simply prohibitive in many processes of interest~\cite{lelievre_free_2010}.

Generative models, which seek to directly approximate $\rhoeq$ and sample efficiently from it, do not suffer from the issue of slow mixing because each new sample is statistically uncorrelated from the last.
However, we must optimize a parametric model to approximate the target probability distribution, which itself incurs a computational cost.
Supposing that we build a generative model with parameters $\theta$ for which we can compute exactly the normalized probability density of the samples we generate $\rho_{\theta}$, then the associated Metropolis-Hastings acceptance criterion a newly proposed configuration in a Markov chain $\xb_0, \dots, \xb_k$ is~\cite{gabrie_adaptive_2022},
\begin{equation}
    \textrm{acc}(\xb_k \to \xb_{k+1}) = \min \left[ 1, \frac{\rhoeq(\xb_{k+1})\rho_{\theta}(\xb_{k}) }{\rhoeq(\xb_{k})\rho_{\theta}(\xb_{k+1}) }\right].
\end{equation}
Importantly, this scheme correctly accounts for the proposal distribution and has the transparent outcome that, if the generative model is identical to the Boltzmann distribution, the acceptance probability is unity. 
This discussion assumes, of course, that it is possible to efficiently build a generative model that i) accurately approximates a given target distribution and ii) allows accurate or exact evaluation of $\rho_{\theta}$ for an arbitrary sample. 
Neither of these assumptions should be taken for granted: we will focus primarily on these questions in the present review.


In general, there are two distinct strategies for building a generative model to represent a Boltzmann distribution: \emph{training with data} and \emph{training with energy}, which we discuss in detail below. 



\subsection{Models trained purely with data}

The most widely used strategies for building generative models require training data.
In some sense, when training with data, these problems become supervised learning problems in which a probabilistic model is trained to recapitulate the data and smoothness is exploited to obtain ``nearby'' configurations.
Many modern generative neural networks were designed for image data, where, as we can intuitively recognize, small perturbations do not typically affect our ability to interpret the result. 
However, this is not the case for molecular systems: a single particle-particle overlap or a violation of chemical bonding rules can be catastrophic. 

In the following discussion, we will focus on two closely related generative modeling strategies, namely, denoising diffusion probabilistic models, which we will simply call \emph{diffusion models}, and flow matching models~\cite{lipman_flow_2022, albergo_building_2023, albergo_stochastic_2023} which are also known as stochastic interpolants.  
There are many classes of generative models that we choose not to discuss here, including generative adversarial networks~\cite{goodfellow_generative_2014} and restricted Boltzmann machines~\cite{salakhutdinov_restricted_2007, salakhutdinov_deep_2009} because, while powerful, these models are lesser used in the applications of interest to this review. 

When discussing models trained with data, we will assume that we are given a data set consisting of $n$ points, denoted $\mathcal{D} = \{ \xb_i \}_{i=1}^n$.
While the frameworks are far more general, the applications we have in mind are those for which each point $\xb_i$ is a molecular configuration and is distributed according to a Boltzmann distribution, i.e., $\xb_i \sim \rhoeq.$

\subsubsection{Denoising diffusion models}\label{sec:diffusion}

Diffusion models were initially developed in Ref.~\cite{sohl-dickstein_hamiltonian_2012} using a theoretical framework built from ideas about time-reversed dynamics originally in the nonequilibrium statistical mechanics literature~\cite{crooks_entropy_1999, kurchan_fluctuation_1998, lebowitz_gallavotticohen-type_1999}. 
At a high level, the idea of diffusion models is to define a stochastic process in which we progressively transform samples from a data set into statistical noise.
Among the mostly widely used approaches is to simulate an Ornstein-Uhlenbeck process starting from a data point $\xb$.
Explicitly, for some time interval $[0, T]$, we use
\begin{equation}
    d\Xb_t = - g(t) \Xb_t dt + \sqrt{2 g(t)} d\Wb_t; \quad \Xb_0 = \xb.
    \label{eq:ou}
\end{equation}
Because an Ornstein-Uhlenbeck process converges exponentially fast to Gaussian distribution, for sufficiently large times $T$, the samples are nearly indistinguishable from Gaussian random variables. 

The key insight underlying diffusion models is that, with the collection of trajectories obtained by the noising dynamics~\eqref{eq:ou}, we can learn a representation of the time-reversal of this dynamics and convert noise into samples that resemble the target distribution.
In fact, Fokker-Planck equation associated with the stochastic process defined in~\eqref{eq:ou} has an exact time-reversal~\cite{anderson_reverse-time_1982,song_score-based_2022} which leads to the unknown reverse process,
\begin{equation}
    d\tilde{\Xb}_t = -g(t)[\tilde{\Xb}_t + 2 \nabla \log \rho_t(\tilde{\Xb}_t)] dt + \sqrt{2 g(t)} d\tilde{\Wb}_t; \quad \tilde{\Xb}_0\sim \mathcal{N}(0,\rm{Id}).
    \label{eq:revou}
\end{equation}
A generative model is constructed by approximating the ``score'' of the intermediate time distributions, $\nabla \log \rho_t$, using a neural network $s(\xb, t; \theta)$.
This procedure is known as score-matching~\cite{song_score-based_2022} and a converged score function enables sampling from the data distribution simply by simulating~\eqref{eq:revou}.
To optimize the score, we minimize an objective~\cite{song_score-based_2022} in which the conditional score appears explicitly,
\begin{equation}
    \mathcal{L}_{\rm score}(\theta) = \EE_t \left[ \lambda(t) \EE_{\xb_t| \xb_0, \xb_0} \left\| s_{\theta}(\xb_t, t) - \nabla \log \rho_t (\xb_t | \xb_0) \right\|^2 \right],
\end{equation}
where the expectations here are computed uniformly in time $t$, with $\xb_0 \sim \rho_0$, and $\xb_t$ sampled from the distribution of stochastic trajectories starting from $\xb_0$ at time $t=0.$
Knowledge of the conditional score enables regression, but an explicit expression for $\log \rho_t (\cdot | \xb_0)$ is only available when the noising SDE is linear, and hence the conditional distribution at intermediate times is Gaussian.
This poses, at the very least, an obstacle to making a physics-informed choice of the noising SDE. 
Diffusion models have already been embraced in a variety of contexts, and while there are several different formalisms (discrete time~\cite{ho_denoising_2020}, continuous time~\cite{song_score-based_2022,vroylandt_likelihood-based_2022}) all these approaches share in common a dynamics that flows samples from a data set to a tractable distribution and a data-dependent parameterization of the inverse dynamics.

Diffusion models have a variety of advantages that make them suitable for working with physical systems. 
One important aspect of the generative process is that the physical invariants can be directly incorporated into the score function.
For example, ~\cite{schneuing_structure-based_2022,weiss_guided_2023} introduced parameterizations of the score that are equivariant with respect to rotations, translations invariant, and permutation invariant for identical atoms.
Ensuring that physical invariances are respected is believed to aid data efficiency and improve generalization, and there is some empirical evidence to support this belief~\cite{sannai_improved_2021}.
Furthermore, it is possible to incorporate physics-based regularizations~\cite{zheng_towards_2023}, which could be useful for treating systems with hard-core repulsive interactions or long-ranged Coulombic interactions.

Ensuring detailed balance within a Metropolis-Hastings scheme requires accounting for asymmetry in the proposal distribution~\cite{frenkel_understanding_2002}.
For generative models, this means that the acceptance criterion for an MCMC proposal is simply
\begin{equation}
    \textrm{acc}(\xb_{k} \to \xb_{k+1}) = \min \left[ 1, \frac{\hat{\rho}(\xb_{k})}{\hat{\rho}(\xb_{k+1})} e^{-\beta \Delta U(\xb_{k+1},\xb_k)} \right].
    \label{eq:mhacc}
\end{equation}
Because the normalization constant cancels in the ratio of generative model probabilities in \eqref{eq:mhacc}, we need only the unnormalized statistical weights associated with the samples, typically called the likelihood in the Bayesian context.
While it is possible to compute exact likelihoods by solving a probability flow ODE associated with the denoising SDE~\eqref{eq:revou}, 
\begin{equation}
    \log \hat\rho(\xb_0) = \log \hat\rho(\xb_T) - \int_0^T g(t) \bigl( d + \nabla \cdot s_{\theta}(\xb_t,t) \bigr) dt,
    \label{eq:pfode}
\end{equation}
where $d$ is the dimensionality, this equation can be difficult to employ in practice because integrating the divergence is computationally expensive, and approximating the trace of the gradient is often the only option~\cite{grathwohl_ffjord_2018}.
One potential strategy for obtaining an unbiased estimator of the partition function of the probability distribution generated by the diffusion model is to approximate the reverse process using a tractable reference process~\cite{vargas_denoising_2023}.

\begin{figure*}
    \centering
    \includegraphics[width=0.75\linewidth]{modular_fig.pdf}
    \caption{Building samplers from physics-informed base distributions is one productive strategy for improving the fidelity of generative models, discussed in Sec.~\ref{sec:nfphys}}
    \label{fig:modular}
\end{figure*}

\subsubsection{Flow-based models}\label{sec:flowmatch}

The idea of transforming samples from an ``easy'' distribution, like a Gaussian, into intricately structured random variables has also motivated more direct approaches.
Normalizing flows~\cite{tabak_density_2010,rezende_variational_2015} seek to transform a tractable initial distribution into samples with high probability in the target distribution with a single map.
Such a transformation, which depends on optimizable parameters $\theta$ and we denote $T_{\theta}$, generates samples by ``pushing forward'' samples from the initial distribution. 
Importantly, normalizing flows are chosen to be invertible functions, meaning that the probability of samples generated by transforming noise with $T_{\theta}$ can be evaluated exactly.
Explicitly,
\begin{equation}
\begin{aligned}
    \xb &= T_{\theta}(\zb) \quad \zb \sim \rho_0, \\
    \hat \rho_\theta(\xb) &= \rho_0(T_{\theta}^{-1}(\xb)) |\nabla T_{\theta}^{-1}(\xb)|,
\end{aligned}
\end{equation}
where $\rho_0$ is the tractable initial distribution and the notation $|\nabla f|$ denotes the determinant of the Jacobian, which arises simply from change of variables. 
Among the most widely used normalizing flow architectures are RealNVP coupling layers~\cite{dinh_density_2017} and rational quadratic splines~\cite{durkan_neural_2019}.
The requirement of invertibility is a strong constraint, and the currently available normalizing flow models can be challenging to optimize~\cite{grathwohl_ffjord_2018,gabrie_adaptive_2022}.

An alternative strategy for building invertible maps exploits the \emph{neural ordinary differential equations}~\cite{chen_neural_2018} framework.
The free-form Jacobian of continuous dynamics (FFJORD) algorithm exploits the classical adjoint state method to optimize a parametric ordinary differential equation using a maximum likelihood loss function. 
In some examples, normalizing flows built with continuous-time ODEs add significant flexibility to models compared with those built on coupling layers, which have a restricted functional form.
This approach is, however, limited computationally due to the requirement that the ODE be solved numerically for sample generation. 
Furthermore, computing exact likelihoods is not tractable for high-dimensional systems because the trace of the Jacobian matrix requires $\mathcal{O}(d^2)$ operations in dimension $d$ at every step of the ODE solve.
To mitigate this, the trace is typically estimated using the Hutchinson trace estimator~\cite{hutchinson_stochastic_1989}, but the variance can be problematic in practice. 

Recent works that seek to extend and enhance the framework of continuous normalizing flows have introduced flow matching~\cite{liu_flow_2022,lipman_flow_2022} or stochastic interpolants~\cite{albergo_building_2023, albergo_stochastic_2023}.
A representation of the direct transformation between two distributions is parameterized using only a vector field $v_{\theta}: \RR^d\times[0,1]\to \RR^d$ and samples from the target distribution $\rho_1$ are obtained by solving the initial value ODE,
\begin{equation}
    \dot \xb_t = v_{\theta}(\xb,t), \quad \xb_0 \sim \rho_0.
\end{equation}
In this generative modeling framework, the transformation between the tractable Gaussian distribution and the target is first built from a sequence of simple intermediate distributions for which the associated flow field can be specified by linear interpolants~\cite{lipman_flow_2022, liu_flow_2022, albergo_building_2023}, 
\begin{equation}
    v(x_0, x_1, t) = \alpha(t) x_0 + \beta(t) x_1 + \gamma(t) z,
\end{equation}
where $\alpha(0)=1$ and $\beta(0)=0$, $\alpha(1)=0$ and $\beta(1)=1$.
The interpolating function can be made noisy by choosing a non-zero function $\gamma(t)$ such that $\gamma(0)=0$ and $\gamma(1)=0$ and letting $z$ be a normally distributed random variable.
The time derivative of the interpolant $v$ provides a regression objective, 
\begin{equation}
    \mathcal{L}_{\rm flow}(\theta) = \EE_{t} \EE_{\xb_0, \xb_1, \zb} \left( \frac12 |v_\theta(\xb, t)|^2 - \dot v(\xb_0, \xb_1) \cdot v_{\theta}(\xb, t) \right).
\end{equation}
These expectations can be evaluated explicitly using a data set and samples from the easily sampled distribution $\rho_0$.

These models share many commonalities with diffusion models, but differ in several potentially important ways.
For diffusion models, the noising dynamics~\eqref{eq:ou} does not completely lose memory of its initial condition in a finite time, meaning that the noise samples are not exactly Gaussian distributed, though there have been recent efforts to ameliorate this problem~\cite{shi_diffusion_2023}.
Because flow-matching models learn a transformation directly between a noise distribution and the target distribution, no approximation is needed to use these models as generative models. 
Furthermore, the relatively implicit objective function used in score-matching constrains the noising dynamics: it will be computationally efficient only if~\eqref{eq:ou} is linear.
Flow-matching models, on the other hand, are optimized using regression directly on the learned velocity field. 
For modeling physical systems, this means that natural regularization terms and constraints can be directly incorporated into the entire generative process. 
While these models have not yet been used widely for physical data, these properties suggest that they could provide a productive avenue for sampling molecular systems. 
However, like diffusion models, likelihood estimation is difficult. 
Computing the likelihood of samples generated from the learned velocity field requires an integral analogous to \eqref{eq:pfode} which will not be tractable in high-dimensional systems.


\subsubsection{Generic challenges with data-based training}\label{sec:databased}

Both diffusion models and flow-based models are trained purely with data, which means that we have not exploited knowledge of the statistical likelihood---i.e., the energy---at all. 
While we can only speculate at this point, we believe that training with the energy function will provide much-needed information in cases where the energy is \emph{rugged}.
Some recent theoretical work has established that inference using diffusion models is statistically \emph{hard} for high-dimensional spin glass models~\cite{ghio_sampling_2023}, but it is not clear whether explicitly incorporating information from the model, or alternatively using an adaptive Monte Carlo strategy~\cite{gabrie_efficient_2021, gabrie_adaptive_2022} would improve the performance.
Along a similar line of inquiry, normalizing flow-based MCMC strategies have been shown to fail for complex constraint satisfaction problems~\cite{ciarella_machine-learning-assisted_2023}.
It should be emphasized that these glassy systems are extremely challenging to sample with \emph{any} algorithm, whether or not it involves machine learning.
Furthermore, data-based generative models are not manifestly useful for the physical sciences unless they can meaningfully generalize to samples far from the training set; it is unclear at present how much data might be necessary for physical systems to achieve good generalization.


\subsection{Training flow-based models with statistical likelihoods} \label{sec:nfs}

Because computing statistical likelihoods is straightforward by construction for normalizing flow models, it is also possible to use a maximum likelihood approach to train the model. 
To do, we use the ``forward'' KL divergence as the loss function
\begin{equation}
\begin{aligned}
    D_{\rm KL}(\hat\rho_{\theta} \| \rhoeq) &= \int \log \frac{\hat\rho_{\theta}(\xb)}{\rhoeq(\xb)} \hat\rho_{\theta}(\xb) d\xb \\
    &= \EE_{\hat\rho_\theta} \left( \log \hat\rho_{\theta} + \beta U(\xb) \right)
    \label{eq:energy_kl}
\end{aligned}
\end{equation}
The appeal of this training modality is that it requires, in principle, \emph{no data}.
The samples are generated by pushing forward random variables from the tractable base distribution $\rho_0$ with the parametric map $T_{\theta}$.
In the context of thermal physical systems, the configurations are Boltzmann distributed, and the statistical likelihood is the unitless temperature-scaled energy $-\beta U.$
For this reason, we also refer to this training procedure as \textit{energy-based} training.

Energy-based training of generative models offers the tantalizing possibility of discovering new molecular configurations while training a model. 
Unfortunately, the objective \eqref{eq:energy_kl} fails in practice due to a phenomenon called \textit{mode collapse}. 
Mode collapse is characterized by the probability mass of the distribution parametrized by the normalizing flow concentrating on a single metastable state.
This phenomenon is illustrated in Fig.~\ref{fig:models} (c).
The propensity of energy-based training to suffer from mode collapse can be controlled to some extent with regularization~\cite{del_debbio_efficient_2021}.
However, even systematic alterations to the loss function do not seem sufficient to avoid mode collapse, though they can mitigate the effect to some extent~\cite{felardos_designing_2023}.

A more reliable method for avoiding mode collapse is to retain probability mass in known modes during training~\cite{gabrie_adaptive_2022}.
While this strategy seems to work in general settings, it requires \emph{a priori} knowledge of the modes.
Mode discovery likely requires direct sampling from physics-based models.
Alternatively, through the means described in the following sections, it may be possible to augment generative models with physical models for mode discovery as discussed in Sec.~\ref{sec:nfphys}. 

\subsection{Adapting normalizing flow models for physical systems}

While it was apparent that normalizing flows could be used as a transition density (also called generation probabilities) in a Markov chain Monte Carlo algorithm~\cite{rezende_variational_2015}, No\'{e} et al.~\cite{noe_boltzmann_2019} were the first to show that this approach could be used in the context of physics-based models of molecular systems, such as proteins.
Alternative machine learning approaches for parameterizing transition densities have also been proposed, including strategies based on Hamiltonian Monte Carlo~\cite{levy_generalizing_2018} and nonlinear independent component estimators~\cite{song_-nice-mc_2017}, but these have not been thoroughly explored for condensed phase systems like proteins. 
In Ref.~\cite{noe_boltzmann_2019}, the strategy requires a large training dataset, because a data-dependent Gaussianization, known as PCA whitening, is used to preprocess the data. 
This approach reduces the learning burden on the normalizing flow because the target distribution has been transformed to be approximately Gaussian. 

A set of subsequent investigations provided a more exacting test of the statistical accuracy of sampling strategies built on density estimation with normalizing flows. 
While atomic crystals have low conformational diversity by comparison with condensed phase biomolecules, exact computation of the absolute free energy is possible due to the now classic Frenkel-Ladd thermodynamic integration approach~\cite{frenkel_new_1984}.  
Work by Wirnsberger and co-workers~\cite{wirnsberger_normalizing_2021, wirnsberger_estimating_2023} and others~\cite{ahmad_free_2021} compared Frenkel-Ladd to sampling with normalizing flows, showing that machine-learned samplers can provide quantitatively accurate estimates of the absolute free energy.
These works introduced an elegant parameterization of a normalizing flow in which the tractable distribution was chosen to be a mixture of truncated Gaussians at the centers of the atomic lattice. 
Recent extensions of this framework for the isothermal-isobaric ensemble~\cite{wirnsberger_estimating_2023} allow for the simultaneous generation of atomic positions and box vectors.
While the results of this approach are again extremely accurate, in both cases, the computational effort associated with training is significant.

The notion of choosing an initial or ``base'' distribution that captures essential physical properties of the system, which we call an informed base distribution, can be essential. 
For example, \cite{gabrie_adaptive_2022} demonstrates that choosing a reference process for a stochastic field theory that does not have the same fluctuation spectrum can lead to mutually singular probability distributions, meaning that no proposal from the normalizing flow will ever be selected.
This basic principle has been leveraged by constructing data-dependent Gaussianization maps to make the data distribution approximately Gaussian, as previously mentioned~\cite{noe_boltzmann_2019}.
Data dependent base distributions may offer a route to developing more expressive generative models, such as those based on rotameric libraries~\cite{dunbrack_jr_bayesian_1997, dunbrack_rotamer_2002}.
A strategy based on this principle is illustrated in Fig.~\ref{fig:modular}.
The relatively small number of modes energetically accessible to a given side chain has inspired a number of efforts to sample these side chains in a data-driven way, for example, in Ref.~\cite{jumper_rapid_2017}.

Preserving appropriate physical invariants, for example, invariance to permutation symmetry and periodic boundary conditions also requires additional consideration.
Many architectures now use attention mechanisms~\cite{vaswani_attention_2017} to obtain permutation invariant representations~\cite{wirnsberger_targeted_2020}.
Similar considerations have been used to develop continuous normalizing flows~\cite{satorras_en_2021-1} that preserve $E(n)$ group symmetries. 
For diffusion models, these considerations are significantly easier because any function can be employed as the score function, including well-established equivariant neural networks like SchNet~\cite{schutt_schnet_2018}. 

While diffusion models~\ref{sec:diffusion} and flow-matching models~\ref{sec:flowmatch} are not typically trained with data-free, maximum likelihood objectives, a number of recent works have sought to additional physical information into the training procedure.
For example, Lai et al.~\cite{lai_fp-diffusion_2023} proposed using a physics-informed neural network style loss function~\cite{karniadakis_physics-informed_2021} to ensure that the Fokker-Planck equation associated with the diffusion process is satisfied.
A more explicit effort to incorporate a physical prior into the loss function was recently used in Ref.~\cite{zheng_towards_2023}, which employs a pre-training step to match the score function with the atomistic forces.


\section{Augmenting mode discovery with coarse-grained models}\label{sec:nfphys}


The dual problem of mode collapse and lack of data requires fundamentally different approaches for building generative models to sample thermodynamic ensembles.
Several recent works have sought to enhance sampling by carrying out dynamics in a ``latent'' space and subsequently recovering the atomistic molecular configuration using a generative model.
In this section, we argue that dimensionality reduction through physics-based coarse-graining~\cite{noid_multiscale_2008, jin_bottom-up_2022} is a scalable strategy for latent space sampling that can avoid mode collapse.

\subsection{Coarse-graining with generative models}

Sidky et al.~\cite{sidky_molecular_2020} were among the first to use a latent space sampling approach for biophysical dynamics.
They trained a generative adversarial network from molecular dynamics trajectory data and provided a proof-of-concept demonstration of data recovery.
However, because generative adversarial networks are not suitable for likelihood estimation or inversion, it is not possible to metropolize or reweight the sampling, so reference data is necessary.
Alternative neural network architectures, such as variational autoencoders, similarly cannot be used with an assurance of Boltzmann statistics, but provide more control over the latent space~\cite{wang_coarse-graining_2019}.
While the specific task in both of these cases is dimensionality reduction for coarse-graining, similar strategies have also been used to develop reaction coordinates~\cite{ribeiro_reweighted_2018}.

More recently, we have combined coarse-grained models with generative models as a strategy to accelerate the rate of mixing between metastable states, potentially discovering new metastable basins~\cite{chennakesavalu_ensuring_2023}.
This approach employs ``backmapping'', i.e., a deterministic or stochastic map that reconstructs the full atomistic state from a coarse-grained configuration~\cite{dunbrack_jr_bayesian_1997, bower_prediction_1997}.
Importantly, Ref.~\cite{chennakesavalu_adaptive_2023} uses invertible normalizing flows to carry out the backmapping, meaning that the conditional probability of the backmapped configuration can be computed exactly and hence reweighted to a Boltzmann distribution. 
Similar techniques have been proposed to bridge levels of electronic structure theory~\cite{maier_bypassing_2022}.

Combining a physics-based coarse-grained model with a \emph{conditional} generative model reduces the computational burden on the generative model in two important ways.
First, because the coarse-graining can be constructed to remove degrees of freedom that fluctuate minimally or are approximately Gaussian in the fine-grained space, the target data distribution can be comparably ``simple''. 
If the generative model needs only to reconstruct Gaussian degrees of freedom, then the learned mapping can itself be simple. 
Secondly, by incorporating a physics-based model, the exploratory tendencies of the model will be constrained to a physically realistic manifold. 

The idea of incorporating auxiliary dynamics to combine sampling with generative models and existing Monte Carlo samplers has its origins in research on adaptive Monte Carlo methods~\cite{del_moral_adaptive_2012}.
Previous work on this topic~\cite{gabrie_efficient_2021, gabrie_adaptive_2022} uses generation moves that alternate between traditional MCMC moves and learned generation with normalizing flows.
Nevertheless, many questions remain about how to best incorporate strong physical priors into generative models, including: Which architectures are best suited to pre-conditioning with physical information? How aggressively should one coarse-grain the original model? In what way should we couple coarse-graining and generative modeling?

Indeed, as coarse-grained models themselves benefit from increasingly powerful supervised learning~\cite{wang_machine_2019, husic_coarse_2020} we should remain optimistic that the states explored in the coarse-grained simulations will correspond well with physically meaningful metastable states of the fine-grained system. 
Leveraging the power of these models and their improved fidelity to the physical free energy surface will constrain generative models in an important and useful way. 
While Ref.~\cite{chennakesavalu_ensuring_2023} takes a first step to faithfully and rigorously couple these two paradigms, the transformative capabilities of generative models will rely on algorithmic, computational, and conceptual developments to extend this work towards true generalization. 


\bibliographystyle{alpha} 
\bibliography{refs}

\end{document}
